
'''
#buat modif retrain tunning model
#buat folder data_trainnya dulu 
#dataset buat trainny ada di drive https://drive.google.com/drive/folders/1USqG9V60kiHIkNOLBV-Jrr1nXtUzbzGK?usp=drive_link


import os
import torch
from torchvision import datasets, transforms
from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer
from transformers import DefaultDataCollator
from datasets import load_dataset, DatasetDict
from PIL import Image
from huggingface_hub import login

# Login Hugging Face
hf_token = os.getenv("HF_TOKEN")
login(token=hf_token)

# Konfigurasi dasar
model_checkpoint = "yangy50/garbage-classification"
data_dir = "data_train/garbage_classification"
output_dir = "./garbage_model_finetuned"

# Load processor dan model
processor = AutoImageProcessor.from_pretrained(model_checkpoint)
model = AutoModelForImageClassification.from_pretrained(
    model_checkpoint,
    ignore_mismatched_sizes=True  # untuk penyesuaian jumlah kelas baru
)

# Cek jumlah label
labels = sorted(os.listdir(data_dir))
label2id = {label: idx for idx, label in enumerate(labels)}
id2label = {idx: label for label, idx in label2id.items()}

# Update config label
model.config.label2id = label2id
model.config.id2label = id2label

# Preprocessing transform
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=processor.image_mean, std=processor.image_std),
])

# Load dataset dari ImageFolder
dataset = datasets.ImageFolder(root=data_dir, transform=transform)

# Split train/val
torch.manual_seed(42)
train_size = int(0.9 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

# Data Collator
data_collator = DefaultDataCollator()

# Training arguments
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir=f"{output_dir}/logs",
    load_best_model_at_end=True,
    push_to_hub=True,
    hub_model_id="fenti/garbage-classification-finetuned",
    hub_token=hf_token
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=processor,
    data_collator=data_collator,
)

# Fine-tune
trainer.train()

# Push ke HuggingFace Hub
trainer.push_to_hub()
'''
